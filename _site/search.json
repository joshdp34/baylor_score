[
  {
    "objectID": "by-statsds-topic.html",
    "href": "by-statsds-topic.html",
    "title": "Modules By Topic",
    "section": "",
    "text": "American Ninja Warrior - Kaplan-Meier Survival Analysis\n\n\n\n\n\n\n\nKaplan-Meier\n\n\nLog Rank test\n\n\nNonparametric tests\n\n\n\n\nExploring Survival Analysis using the Kaplan-Meier method with American Ninja Warrior data.\n\n\n\n\n\n\nJun 4, 2025\n\n\nJonathan Lieb, Rodney X. Sturdivant\n\n\n\n\n\n\n  \n\n\n\n\nMLB Injuries - Introductory Time Series Analysis\n\n\n\n\n\n\n\nTime series plots\n\n\nTime series decomposition\n\n\nResidual analysis\n\n\nSimple forecasting\n\n\n\n\nExploring MLB injury data through time series analysis and forecasting.\n\n\n\n\n\n\nNov 26, 2024\n\n\nJonathan Lieb\n\n\n\n\n\n\n  \n\n\n\n\nWhat‚Äôs the prime age of an MLB player?\n\n\n\n\n\n\n\nData wrangling\n\n\nDplyr basics\n\n\nTidyverse\n\n\n\n\nUse data wrangling skills to explore the prime age of MLB players\n\n\n\n\n\n\nJul 26, 2024\n\n\nJazmine Gurrola, Joseph Hsieh, Dat Tran, Katie Fitzgerald\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "baseball/mlb_injuries/index.html",
    "href": "baseball/mlb_injuries/index.html",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "",
    "text": "Facilitation notes\n\n\n\n\n\n\nThis module would be suitable for an in-class lab or take-home assignment in an intermediate statistics course.\nIt assumes a familiarity with the RStudio Environment and R programming language.\nStudents should be provided with the following data file (.csv) and Quarto document (.qmd) to produce visualizations and write up their answers to each exercise. Their final deliverable is to turn in an .html document produced by ‚ÄúRendering‚Äù the .qmd.\n\nMonthly Injury Data\nTommy John Surgeries Data\nStudent Quarto template\n\nPosit Cloud (via an Instructor account) or Github classroom are good options for disseminating files to students, but simply uploading files to your university‚Äôs course management system works, too.\nThe data for the mlb_injuries_monthly.csv file was derived from data found on prosportstransactions.com. The original data from the site contained rows of observations showing each transaction that involved sending a player to the injured list or bringing a player off of the injured list. The data was then filtered to include only times when a player was sent to the injured list and aggregated by month. The 2000 season was originally included in the data, but was removed after incomplete data was found for that season.\nThe data for the tj_surgeries_mlb_milb.csv file was derived from data accumulated by @MLBPlayerAnalys over many years. The original data lists any reported Tommy John surgery for a player now playing in the MLB or MiLB. This data was aggregated by year to produce the data used in this module."
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#classical-decomposition",
    "href": "baseball/mlb_injuries/index.html#classical-decomposition",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "Classical Decomposition",
    "text": "Classical Decomposition\nThere are two common methods for decomposing time series data: classical decomposition and STL decomposition. In this module we will focus on classical decomposition.\nClassical decomposition (additive) has four main steps:\n\nComputing a trend-cycle component (\\(T_t\\)) using a moving average. A moving average is a technique for smoothing time series data by averaging the values of neighboring points. This helps to remove short-term fluctuations and highlight longer-term trends.\nComputing a series without the trend-cycle component (\\(y_t - T_t\\)).\nEstimating the seasonal component (\\(S_t\\)) by averaging the values from the detrended series for the season.\nComputing the remainder component (\\(R_t\\)) by subtracting the trend-cycle and seasonal components from the original series. \\(R_t = y_t - T_t - S_t\\)\n\nWe can use the classical_decomposition function inside of the model() function to decompose our time series data.\n\ninjuries |&gt; \n  model(classical_decomposition(Count)) |&gt;\n  components() |&gt;\n  autoplot() +\n  labs(title = \"Classical Additive Decomposition of Monthly MLB Injury Counts\")\n\n\n\n\nClassical multiplicative decomposition works similarly to the additive decomposition, but with a few key differences.\n\nThe detrended series is computed as \\(y_t / T_t\\) instead of \\(y_t - T_t\\).\nThe seasonal component is estimated as \\(S_t = y_t / T_t\\) instead of \\(y_t - T_t\\).\nThe remainder component is computed as \\(R_t = y_t / (T_t \\times S_t)\\) instead of \\(y_t - T_t - S_t\\).\n\n\n\nNOTE: Ideally, after doing a decomposition, the remainder component should be white noise.\nClassical multiplicative decomposition can be used by setting the type argument to \"multiplicative\" in the classical_decomposition() function."
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#stl-decomposition",
    "href": "baseball/mlb_injuries/index.html#stl-decomposition",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "STL Decomposition",
    "text": "STL Decomposition\nSeasonal and Trend decomposition using Loess (STL) is a more advanced method for decomposing time series data. It is more flexible than classical decomposition and can handle any type of seasonality, not just monthly or quarterly. It also allows the user to control the length of the smoothing window for the trend-cycle. Lastly, it is more robust to outliers so that they do not affect the trend and seasonal estimates as much.\nBelow is an example of how to use the STL() function to decompose the time series data.\n\ninjuries |&gt; \n  model(STL(Count ~ trend(window = 21)+ \n            season(window = \"periodic\"),\n            robust = TRUE)) |&gt;\n  components() |&gt;\n  autoplot() +\n  labs(title = \"STL Additive Decomposition of Monthly MLB Injury Counts\")\n\n\n\n\n\n\nTIP: The window argument in the trend() function controls the length of the smoothing window for the trend-cycle. The larger the window, the smoother the trend. This value should always be an odd number so that a central point can be used. trend(window = 21) is a common choice for monthly data. This relatively large window size helps to prevent the trend from being influenced by short-term fluctuations in just a single year, but rather capture long-term trends and cycles.\nTIP: The window argument in the season() function controls how many years the seasonal component should be estimated over. The default value is 11. When the seasonal window is set to periodic season(window = \"periodic\"), it is the equivalent setting the window to all of the data. When periodic is used, the seasonal component is assumed to be the same each year. The seasonal window argument should always be an odd number or ‚Äúperiodic‚Äù.\n\n\n\n\n\n\nExercise 3: Changing Decomposition Types\n\n\n\n\n\nCreate a classical multiplicative decomposition of the monthly MLB injury counts. How do the components differ from the additive decomposition? Which decomposition method‚Äôs remainder component looks more like white noise (classical additive or classical multiplicative)?\nCreate an STL decomposition of the monthly MLB injury counts with a shorter length for the trend smoothing window. How does the decomposition change with a shorter trend smoothing window? Particularly, how does the trend component change?\n\n\n\n\nNOTE: You can actually forecast with decomposition as well. If you‚Äôd like to learn more about this click here"
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#the-mean-method",
    "href": "baseball/mlb_injuries/index.html#the-mean-method",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "The Mean Method",
    "text": "The Mean Method\nAn extremely simple method for forecasting is the mean method. This method forecasts the next observation as the average of all the observations in the training data. This method will produce a flat forecast that is equal to the mean of the training data. The mean method is useful when the data doesn‚Äôt have a trend or seasonality."
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#the-naive-method",
    "href": "baseball/mlb_injuries/index.html#the-naive-method",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "The Naive Method",
    "text": "The Naive Method\nThe naive method is another simple forecasting method. It forecasts the next observation as the value of the last observation in the training data. This method will produce a flat forecast that is equal to the last observation in the training data. The naive method is useful when the data appears to be random."
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#seasonal-naive-method",
    "href": "baseball/mlb_injuries/index.html#seasonal-naive-method",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "Seasonal Naive Method",
    "text": "Seasonal Naive Method\nThe seasonal naive method is similar to the naive method, but it forecasts the next observation as the value from the same season in the previous year. This method will produce a repeating pattern of forecasts that are equal to the observations from the same season in the previous year. (Basically forever repeating the last year‚Äôs pattern). The seasonal naive method is useful when the data has a strong seasonal pattern but no trend."
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#drift-method",
    "href": "baseball/mlb_injuries/index.html#drift-method",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "Drift Method",
    "text": "Drift Method\nThe drift method is a simple forecasting method that assumes a linear trend in the data. It forecasts the next observation as the value of the last observation plus the average change between observations. This method will produce a forecast that will continue on a linear trend from the first observation and through the last observation in the training data. The drift method is useful when the data has a linear trend but no seasonality.\n\n\n\n\n\n\nExercise 4: Basic Forecasting Methods\n\n\n\n\n\n\n\n\n\nWhich of the above time series plots would be best forecasted using the mean method?\nWhich of the above time series plots would be best forecasted using the seasonal naive method?\nWhich of the above time series plots would be best forecasted using the drift method?\n\nUse the following plots of the monthly MLB injury counts and forecasts to answer the following questions.\n\n\n\n\n\n\nAfter looking at the forecasts from the mean, naive, seasonal naive, and drift methods for the MLB injury data, which method appears to be the best forecast?\nWhich ones appear to be the worst forecasts?\nWhat is one major issue seen in all of the forecasts in regards to their prediction intervals?"
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#residuals",
    "href": "baseball/mlb_injuries/index.html#residuals",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "Residuals",
    "text": "Residuals\nChecking the residuals of a model is one of the most effective ways to see how well the model is performing. Residuals are the differences between the observed values and the values predicted by the model. \\(e_t = y_t - \\hat{y}_t\\)\n\n\nIt is important to note that when we are talking about residuals in this module that we are referring to the innovation residuals. Most of the time innovation residuals are the same as regular residuals, such as with our seasonal naive model. Innovation residuals are the residuals that are left over after accounting for changes made to the data such as transformations or differencing. These residuals are the ones that are used to check the model assumptions and to evaluate the model‚Äôs performance.\nThere are 3 main things to look at when evaluating residuals:\n\nDo the residuals appear to be white noise? Remember that white noise has a mean of 0, constant variance, and shows no obvious patterns. This can be checked by looking at a time plot of the residuals.\nAre the residuals normally distributed? This can be checked by looking at a histogram of the residuals or by using a normal probability plot.\nAre the residuals uncorrelated? This can be checked by looking at the ACF plot of the residuals. There are also statistical tests that can be used to check for autocorrelation in the residuals such as the Ljung-Box test. We can use the Box.test() function in R to perform this test.\n\n\n\nThe formula for the test statistic for a Ljung-Box test is:\n\\[Q^{*} = T(T+2) \\sum_{k=1}^{l} \\frac{r^2_k}{T-k}\\]\nwhere:\n\n\\(T\\) is the number of observations\n\\(l\\) is the max number of lags\n\\(r_k\\) is the sample autocorrelation at lag \\(k\\)\n\nThe null hypothesis for the Ljung-Box test is that the data is not distinguishable from white noise. If the p-value is less than the significance level, we reject the null hypothesis and conclude that the data is autocorrelated.\nThankfully there is a very easy way to check all of these at once in R using the gg_tsresiduals() function.\n\n\n\n\n\n\nExercise 5: Residuals Analysis\n\n\n\nBelow is code that will create a time plot, histogram, and ACF plot of the residuals from the seasonal naive method.\n\nsnaive_mod &lt;- injuries |&gt;\n  model(SNAIVE(Count ~ lag('year')))\n  \nsnaive_mod |&gt; \n  gg_tsresiduals()\n\n\n\n\n\nDo the residuals appear to be white noise?\nWhat stands out about the time plot of the residuals?\nDoes the histogram of the residuals appear to be normally distributed?\nAre the residuals uncorrelated? What lag(s) show the most significant autocorrelation and what could this mean for the model?"
  },
  {
    "objectID": "baseball/mlb_injuries/index.html#testing-and-training-for-point-estimate-evaluations",
    "href": "baseball/mlb_injuries/index.html#testing-and-training-for-point-estimate-evaluations",
    "title": "MLB Injuries - Introductory Time Series Analysis",
    "section": "Testing and Training for Point Estimate Evaluations",
    "text": "Testing and Training for Point Estimate Evaluations\nIf you want to evaluate the point estimates of a model, you can use a testing and training split. This involves training the model on the first part of the data and then testing the model on the second part of the data. This allows you to see how well the model can forecast future observations.\nFor this example, we will split the data into a training set that contains the first 75% of the data and a testing set that contains the last 25% of the data. This means we will train the models on the data from January 2001 to December 2017 and test the models on the data from January 2018 to December 2023.\n\n\nTIP: 75-25 is a common split for training and testing data, but you can use any split that makes sense for your data. Generally the more data you have, the less percentage you need for testing. Other common splits are 70-30 or 80-20.\n\ntraining &lt;- injuries |&gt; filter(year(Month) &lt; 2018)\ntesting &lt;- injuries |&gt; filter(year(Month) &gt;= 2018)\n\nNow that we have our training and testing data, we can fit the models to the training data and then forecast the testing data.\n\ninjury_fit &lt;- training |&gt; \n  model(mean = MEAN(Count),\n        naive = NAIVE(Count),\n        snaive = SNAIVE(Count ~ lag('year')),\n        drift = RW(Count ~ drift()))\n\ninjury_forecasts &lt;- injury_fit |&gt; \n  forecast(new_data = testing)\n\n\n\nTIP: You can fit multiple models at once by using the model() function as seen in the code to the left. The values to the left of the = are the names we are giving to the models and the values to the right of the = are the models we are fitting to the data.\nTIP: When using the forecast() function, you can specify the new data you want to forecast by using the new_data argument. In this case, we are forecasting for the testing data.\nLet‚Äôs visualize the forecasts from the training data and compare them to the testing data.\n\ninjury_forecasts |&gt;\n  autoplot(injuries, level = NULL) +\n  labs(title = \"Forecasting Methods for Monthly MLB Injury Counts\")+\n  guides(color = guide_legend(title = \"Forecast\"))\n\n\n\n\nThe seasonal naive method certainly appears to be the best forecast.\nWe can also evaluate these models using the accuracy() function. This function calculates a variety of accuracy measures for the forecasts, including the mean absolute error, root mean squared error, mean absolute percentage error, and more.\n\nThe mean absolute error (MAE) is the average of the absolute errors between the forecasts and the actual values. It is a measure of the average magnitude of the errors in the forecasts. We want this value to be as close to 0 as possible.\n\n\nThe formula for the mean absolute error is: \\[\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} | y_i - \\hat{y}_i |\\] where \\(y_i\\) is the actual value and \\(\\hat{y}_i\\) is the forecasted value.\n\nThe root mean squared error (RMSE) is the square root of the average of the squared errors between the forecasts and the actual values. It is a measure of the standard deviation of the errors in the forecasts. We want this value to be as close to 0 as possible.\n\n\nThe formula for the root mean squared error is: \\[\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\\] Where \\(y_i\\) is the actual value and \\(\\hat{y}_i\\) is the forecasted value.\n\nThe mean absolute percentage error (MAPE) is the average of the absolute percentage errors between the forecasts and the actual values. It is a measure of the accuracy of the forecasts. We want this value to be as close to 0 as possible.\n\n\nThe formula for the mean absolute percentage error is: \\[\\text{MAPE} = \\frac{100}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right|\\] where \\(y_i\\) is the actual value and \\(\\hat{y}_i\\) is the forecasted value.\n\nThe code below displays the accuracy measures for the forecasts.\n\naccuracy(injury_forecasts, testing)\n\n# A tibble: 4 √ó 10\n  .model .type    ME  RMSE   MAE   MPE  MAPE  MASE RMSSE  ACF1\n  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 drift  Test   86.0 122.   86.0   Inf   Inf   NaN   NaN 0.698\n2 mean   Test   36.2  93.9  79.2  -Inf   Inf   NaN   NaN 0.698\n3 naive  Test   86.0 122.   86.0   100   100   NaN   NaN 0.698\n4 snaive Test   14.3  58.4  34.5  -Inf   Inf   NaN   NaN 0.424\n\n\nThis confirms that the seasonal naive method is the best forecast, as it has the lowest MAE and RMSE values. The MAPE is shown at -Inf to Inf because some of the actual values are 0, which causes the percentage error to be infinite."
  },
  {
    "objectID": "_team.html",
    "href": "_team.html",
    "title": "üèÖ The Baylor SCORE Team",
    "section": "",
    "text": "Rodney X. Sturdivant\n\n\n\n\n\nRod Sturdivant is a Professor of Statistical Science and Director of the Statistical Consulting Center.\n\n\n\n\n\n\n\n\n\nMichael Gallaugher\n\n\n\n\n\nMichael Gallaugher is an Assistant Professor of Statistical Science.\n\n\n\n\n\n\n\n\n\nJoshua Patrick\n\n\n\n\n\nJoshua Patrick is a Senior Lecturer of Statistical Science.\n\n\n\n\n\n\nWe‚Äôve had numerous students assist in developing both SCORE modules and preparing submissions for the data repository. They are listed by year below.\n\n\n\n\n\n\nStudent Contributors\n\n\n\n\n\nThe following students have assisted in developing SCORE materials\n\nSpring 2023\n\nConnor Bryson\nTony Munoz\n\nSummer 2023\n\nCaleb Skinner\nIan Young\n\nSummer 2024\n\nJonathan Lieb"
  },
  {
    "objectID": "_team.html#faculty",
    "href": "_team.html#faculty",
    "title": "üèÖ The Baylor SCORE Team",
    "section": "",
    "text": "Rodney X. Sturdivant\n\n\n\n\n\nRod Sturdivant is a Professor of Statistical Science and Director of the Statistical Consulting Center.\n\n\n\n\n\n\n\n\n\nMichael Gallaugher\n\n\n\n\n\nMichael Gallaugher is an Assistant Professor of Statistical Science.\n\n\n\n\n\n\n\n\n\nJoshua Patrick\n\n\n\n\n\nJoshua Patrick is a Senior Lecturer of Statistical Science."
  },
  {
    "objectID": "_team.html#students",
    "href": "_team.html#students",
    "title": "üèÖ The Baylor SCORE Team",
    "section": "",
    "text": "We‚Äôve had numerous students assist in developing both SCORE modules and preparing submissions for the data repository. They are listed by year below.\n\n\n\n\n\n\nStudent Contributors\n\n\n\n\n\nThe following students have assisted in developing SCORE materials\n\nSpring 2023\n\nConnor Bryson\nTony Munoz\n\nSummer 2023\n\nCaleb Skinner\nIan Young\n\nSummer 2024\n\nJonathan Lieb"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Baylor SCORE Module Pre-print Repository",
    "section": "",
    "text": "This page contains education materials for the SCORE Network that were created by faculty and students from the Department of Statistical Science at Baylor University and the Department of Mathematics, Physics, & Statistics at Azusa Pacific University."
  },
  {
    "objectID": "index.html#all-modules",
    "href": "index.html#all-modules",
    "title": "Baylor SCORE Module Pre-print Repository",
    "section": "All Modules",
    "text": "All Modules\n\n\n\n\n\n\nBrowse all modules below. Use the sidebar to filter by sport or visit by statistics topic.\n\n\n\n\n\n\n\n  \n\n\n\n\nAmerican Ninja Warrior - Kaplan-Meier Survival Analysis\n\n\n\n\n\n\n\nKaplan-Meier\n\n\nLog Rank test\n\n\nNonparametric tests\n\n\n\n\nExploring Survival Analysis using the Kaplan-Meier method with American Ninja Warrior data.\n\n\n\n\n\n\nJun 4, 2025\n\n\nJonathan Lieb, Rodney X. Sturdivant\n\n\n\n\n\n\n  \n\n\n\n\nMLB Injuries - Introductory Time Series Analysis\n\n\n\n\n\n\n\nTime series plots\n\n\nTime series decomposition\n\n\nResidual analysis\n\n\nSimple forecasting\n\n\n\n\nExploring MLB injury data through time series analysis and forecasting.\n\n\n\n\n\n\nNov 26, 2024\n\n\nJonathan Lieb\n\n\n\n\n\n\n  \n\n\n\n\nWhat‚Äôs the prime age of an MLB player?\n\n\n\n\n\n\n\nData wrangling\n\n\nDplyr basics\n\n\nTidyverse\n\n\n\n\nUse data wrangling skills to explore the prime age of MLB players\n\n\n\n\n\n\nJul 26, 2024\n\n\nJazmine Gurrola, Joseph Hsieh, Dat Tran, Katie Fitzgerald\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#other-preprint-servers",
    "href": "index.html#other-preprint-servers",
    "title": "Baylor SCORE Module Pre-print Repository",
    "section": "Other Preprint Servers",
    "text": "Other Preprint Servers\nThis page links to preprint repositories maintained by members of the SCORE Network. These preprint repositories are created and maintained by faculty and students from the respective institutions. Please note that these materials have not yet completed the required pedagogical and industry peer reviews to become a published module on the SCORE Network. However, instructors are still welcome to use these materials if they are so inclined.\nThe following are actively maintained preprint repositories:\n\nCarnegie Mellon University\nSt.¬†Lawrence University\nBaylor University (This site)\nWest Point"
  },
  {
    "objectID": "index.html#contributing-andor-joining-the-score-network",
    "href": "index.html#contributing-andor-joining-the-score-network",
    "title": "Baylor SCORE Module Pre-print Repository",
    "section": "Contributing and/or Joining the SCORE Network",
    "text": "Contributing and/or Joining the SCORE Network\n\nIf you are interested in contributing to and/or joining the SCORE Network, please check out https://scorenetwork.org/index.html.\nIf you are interested in creating a page similar to this to host your own ‚Äúin development modules‚Äù, please feel free to copy the associated Github repository: https://github.com/iramler/slu_score_preprints."
  },
  {
    "objectID": "index.html#funding-source",
    "href": "index.html#funding-source",
    "title": "Baylor SCORE Module Pre-print Repository",
    "section": "Funding Source",
    "text": "Funding Source\nThe development of the SCORE with Data network is funded by the National Science Foundation (award 2142705)."
  },
  {
    "objectID": "anw/kaplan_meier/index.html",
    "href": "anw/kaplan_meier/index.html",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analysis",
    "section": "",
    "text": "Facilitation notes\n\n\n\n\n\n\nThis module would be suitable for an in-class lab or take-home assignment in an intermediate statistics course.\nIt assumes a familiarity with the RStudio Environment and R programming language.\nStudents should be provided with the following data file (.csv) and Quarto document (.qmd) to produce visualizations and write up their answers to each exercise. Their final deliverable is to turn in an .html document produced by ‚ÄúRendering‚Äù the .qmd.\n\n2021 Stage 1 Finals Data\n2023 Stage 1 Finals Data\nStudent Quarto template\n\nPosit Cloud (via an Instructor account) or Github classroom are good options for disseminating files to students, but simply uploading files to your university‚Äôs course management system works, too.\nThe anw_2021_stage1.csv data is derived largely from americanninjawarriornation.com. Additional columns such as sex were individually researched and added to the data.\nThe anw_2023_stage1.csv data is derived largely from sasukepedia."
  },
  {
    "objectID": "anw/kaplan_meier/index.html#terms-to-know",
    "href": "anw/kaplan_meier/index.html#terms-to-know",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analysis",
    "section": "Terms to know",
    "text": "Terms to know\nBefore proceeding with the survival analysis, let‚Äôs make sure we understand American Ninja Warrior and some of it‚Äôs vocabulary to help us climb our way through this lab.\n\nHow does American Ninja Warrior work?\nAmerican Ninja Warrior is an NBC competition show where participants attempt to complete a series of obstacle courses of increasing difficulty. In a single obstacle course, the competitors must complete a series of obstacles in a row. If they fail an obstacle (usually this happens when they fall into the water below), they are eliminated from the competition. The competitors also have a time limit to complete the course. The competitors are ranked based on how far they get in the course and how quickly they complete it.\nMost of the obstacles are designed to test the competitors‚Äô upper body strength. Some obstacles require balance and agility though.\n\n\nThe warped wall is arguably the most famous, although now least difficult, obstacle on an American Ninja Warrior course. The warped wall is a curved wall that competitors must run up and grab the top of. The warped wall is on every course and is often the final obstacle, although this is not the case on the Finals courses.\nThe warped wall was previously 14 feet and is now 14.5 feet tall. They have even had a 18 foot warped wall on the show.\n\n\n\nWarped Wall\n\n\nImage Source: Dustin Batt, CC BY-SA 2.0, via Wikimedia Commons\nThe obstacles in American Ninja Warrior are all given names. For example, the famed Warped Wall is a curved wall that competitors must run up and grab the top of. The Salmon Ladder is a series of rungs that competitors must move up by jumping and pulling themselves up.\nWatch Enzo Wilson complete the American Ninja Warrior course at the 2021 Finals Stage 1 (Season 13) in the video below.\n\n\n\n\n\n\n\nKey Terms\n\n\n\n\nObstacle: A challenge that competitors must complete to move on in the competition.\nCourse: A series of obstacles that competitors must complete in a row. A typical course has 6-10 obstacles.\nStage: A round of the competition. The competition starts with Stage 1 and progresses to Stage 4.\nTime Limit: The amount of time competitors have to complete the course, often between 2-4 minutes."
  },
  {
    "objectID": "anw/kaplan_meier/index.html#variable-descriptions",
    "href": "anw/kaplan_meier/index.html#variable-descriptions",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analysis",
    "section": "Variable Descriptions",
    "text": "Variable Descriptions\nThe ninja data you‚Äôll be analyzing in this lab provides the individual run information for each ninja in the 2021 Finals Stage 1 (Season 13). The data includes the ninja‚Äôs name, their sex, the obstacle they failed on, and the cause of that failure.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nname\nName of the American Ninja Warrior\n\n\nsex\nSex of the American Ninja Warrior (M/F)\n\n\nobstacle\nThe name of the obstacle the ninja fell on or the last obstacle they completed if they ran out of time or finished the course\n\n\nobstacle_number\nThe obstacle‚Äôs place in the run\n\n\ncause\nWhat caused the ninja to fail (Fall/Time/Complete)\n\n\n\n\n\nNote: It is important to recognize that if the competitor fell on the 7th obstacle the obstacle number is 7. However, if the competitor ran out of time on the 7th obstacle, the obstacle number is 6, since the 6th obstacle was the last time that we could say they either fell or completed an obstacle. This will be important for censoring later. If the competitor completed the course, the obstacle number is set to the last obstacle they completed, which is the 9th obstacle in the 2021 Finals Stage 1 course."
  },
  {
    "objectID": "anw/kaplan_meier/index.html#points-of-confusion",
    "href": "anw/kaplan_meier/index.html#points-of-confusion",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analysis",
    "section": "Points of Confusion",
    "text": "Points of Confusion\nUse the following code to create a table of the obstacle names and their corresponding obstacle numbers. This will help you understand the order of the obstacles in the course.\n\nninja |&gt; \n  distinct(obstacle, obstacle_number)\n\n# A tibble: 10 √ó 2\n   obstacle        obstacle_number\n   &lt;chr&gt;                     &lt;dbl&gt;\n 1 Slide Surfer                  1\n 2 Swinging Blades               2\n 3 Double Dipper                 3\n 4 Jumping Spider                4\n 5 Tire Run                      5\n 6 Dipping Birds                 7\n 7 Warped Wall                   6\n 8 The High Road                 8\n 9 Fly Hooks                     8\n10 Cargo Net                     9\n\n\nIt can be seen quickly that there are 2 obstacles with number 8. The duplicate obstacle number 8 is due to the fact that Stage One of the 2021 Finals allowed a split-decision. This means that competitors could choose between two different obstacles for obstacle 8.\n\n\nRead about the 2021 Stage 1 Split Decision here.\nAdditionally, one competitor Joe Moravsky ran the course twice (falling the first time and completing it the second time). This is because he was the Safety Pass Winner from the previous round. The Safety Pass allows a competitor to run the course again if they fail the first time. This poses some questions about how to handle this observation. We could\n\nInclude both runs in the analysis, treating them as separate observations.\nInclude only the first run in the analysis.\nInclude only the second run in the analysis.\n\nIf we include the second run in the analysis, we are neglecting the fact that Joe Moravsky had already attempted the course once and may have learned from his mistakes.\nIf we include the first run in the analysis, an argument could be made that Moravsky only failed the first time because he knew he had a second chance.\nIn most survival analysis situations, an individual would not be capable of participating twice from the beginning (after all if death were truly the event of interest, it would be safe to say there is no second chance). Therefore, we will only include the first run in the analysis.\nRun the code below to remove the second run from the data.\n\nninja &lt;- ninja |&gt; \n  filter(name != \"Joe Moravsky (Safety Pass)\")\n\nNow that we‚Äôve cleared some of the muddiness, let‚Äôs move on to the fun stuff!"
  },
  {
    "objectID": "anw/kaplan_meier/index.html#censored-data",
    "href": "anw/kaplan_meier/index.html#censored-data",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analysis",
    "section": "Censored Data",
    "text": "Censored Data\nIn survival analysis, we often need to censor data. Censored data occurs when the event of interest has not occurred for some of the observations. For the heart attack example, suppose we follow subjects for 10 years. It is not unlikely that some will not have a second heart attack in that time. It is also possible that we are unable to follow all subjects for the full 10 years. Such data is censored. We have information that the person did not have a second heart attack for a certain amount of time, but we do not have information beyond that time.\nWe could, of course, just exclude these observations and use only those who had a second heart attack. The problem is that this creates potential bias. Suppose those who had treatment A do not have a second heart attack as quickly. More in that group might not have a heart attack in the 10 years. By excluding them we would bias our estimate of the average time until a heart attack for this group to be shorter than it actually is."
  },
  {
    "objectID": "anw/kaplan_meier/index.html#introducing-survival-analysis-using-american-ninja-warrior-data",
    "href": "anw/kaplan_meier/index.html#introducing-survival-analysis-using-american-ninja-warrior-data",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analysis",
    "section": "Introducing Survival Analysis using American Ninja Warrior Data",
    "text": "Introducing Survival Analysis using American Ninja Warrior Data\nIn the heart attack study, we have two key variables. One is the time each subject is observed \\(t\\) = years, which we suppose are only integer values 1, 2, 3,‚Ä¶(technically this would be interval censored data but we will not explore this more advanced topic here). The second is the censoring variable \\(c_i\\), which tells us if we observed the event \\(c_i = 1\\) or not \\(c_i = 0\\).\nOur data for two subjects might look like:\n\n\n\nSubjectTimeCensor1812100\n\n\nWe see that subject 1 was observed for 8 years and had a heart attack at year 8. Subject 2, on the other hand, was observed for the full 10 years and did not have a heart attack.\nTo illustrate survival analysis for the ANW data, we will define the time using the obstacle number. So, instead of time \\(t = 0, 1, 2,...\\) years we will have time \\(n = 0, 1, 2, 3,...\\) obstacles. Note that there is an actual time element to the obstacle course, but for the purpose of learning survival analysis our time is the obstacle number.\nThe event of interest is the failure of a competitor on an obstacle. This is important as it is possible to fail the course itself, for stage 1, if the allotted time expires. However, that is not the outcome of interest here.\nAs with the heart attack example, there are several ways in which the data might be censored. If a competitor completes the course, we do not know how many obstacles they would complete before the event (failing an obstacle) would occur. This is similar to the case where a subject was followed for 10 years without a heart attack; our subject was followed for 9 obstacles (our time variable) without failing an obstacle.\nAn additional possible censoring occurs if the time limit is reached. Again, if that occurs we do not know how many obstacles the competitor would complete before failing one, although we do know that they didn‚Äôt fail on any of the ones they completed before time was up. In the heart attack study this would be similar to a case where we observed a subject for 6 years and they died due to some other cause but never had a heart attack.\n\n\nNote: Another advanced survival model involves analyzing ‚Äúcompeting risks‚Äù if interest is in multiple different outcomes such as death from a heart attack or death from some other cause. In the ANW case, failing an obstacle or running out of time would be competing risks. We will not explore this topic here.\nUse the code below to create a new column called censor in the ninja data that is a binary indicator of whether or not the observation should be censored. This column will be used to indicate whether the data is censored or not.\n\n# Makes a column called censor that is 1 if the competitor failed and 0 if they completed the course or ran out of time\nninja &lt;- ninja |&gt; \n  mutate(censor = if_else(cause %in%  c(\"Complete\", \"Time\"), 0, 1))\n\nThree competitors in the resulting data are:\n\n\n\nnamesexobstacleobstacle_numbercausecensorMeagan MartinFSlide Surfer1Fall1Sean BryanMCargo Net9Complete0Brett SimsMFly Hooks8Time0\n\n\nWe can interpret this data:\n\nMeaghan Martin failed on the first obstacle so the time (obstacle_number) is 1 and censor = 1.\n\nSean Bryan completed the course so the obstacle_number is 9 but censor = 0. We do not know how many obstacles Sean completes before failing one; he moved on to the second stage course but we did not collect data after the first stage. He was censored due to the ‚Äústudy time‚Äù of 9 obstacles.\nBrett Sims ran out of time while on the 9th obstacle. We know he completed 8 obstacles so his ‚Äútime‚Äù is obstacle_number = 8 but we do not know if he would have failed the 9th obstacle or not (or any further obstacles) so his observation is censored."
  },
  {
    "objectID": "anw/kaplan_meier/index.html#estimating-the-survival-function-using-kaplan-meier",
    "href": "anw/kaplan_meier/index.html#estimating-the-survival-function-using-kaplan-meier",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analysis",
    "section": "Estimating the Survival Function using Kaplan-Meier",
    "text": "Estimating the Survival Function using Kaplan-Meier\nThe Kaplan-Meier estimator uses information from all of the observations in the data to provide a non-parametric estimate of the survival function. The estimator considers survival to a certain point in time as a series of steps defined at the observed times.\nIn order to calculate the probability of surviving past a certain point in time (past a certain obstacle in this case), the conditional probability of surviving past that point given that the competitor has survived up to that point must be calculated first.\nThe formula for the conditional probability of surviving past a point in time (\\(t_i\\)) given that the competitor has survived up to that point in time(\\(t_{i-1}\\)) is:\n\n\nNote: This function could also be written as \\(P(T &gt; t_i | T \\geq t_{i-1}) = \\frac{n_i - d_i}{n_i}\\)\n\\(P(T \\geq t_i | T \\geq t_{i-1}) = 1- \\frac{d_i}{n_i}\\)\nWhere:\n\n\\(d_i\\) is the number of competitors that failed at time \\(t_i\\)\n\\(n_i\\) is the number of competitors that were at risk at time \\(t_i\\)\n\nThe Kaplan-Meier estimator is the product of the conditional probabilities of surviving past each point in time up through that point in time.\n\\(\\hat{S}(t) = \\prod_{t_i \\leq t} (1 - \\frac{d_i}{n_i})\\)\n\n\nNote: Censored data does not count in the at risk competitors\nwhere \\(n_i = n_{i-1} - d_{i-1} - c_{i-1}\\)\n\n\\(c_i\\) is the number of competitors censored at time \\(t_i\\)\n\nFor example, we create a data set with 25 competitors and 5 obstacles:\n\n# Setting a seed for reproducibility\nset.seed(123)\n\n# Creating fake data\nfake_data &lt;- tibble(obstacle_number = c(1:5, 2,5), censor = c(rep(1, 5), rep(0, 2))) |&gt; \n  sample_n(25, replace = TRUE)\n\nhead(fake_data)\n\n# A tibble: 6 √ó 2\n  obstacle_number censor\n            &lt;dbl&gt;  &lt;dbl&gt;\n1               5      0\n2               5      0\n3               3      1\n4               2      0\n5               3      1\n6               2      1\n\n\nEach row of the data is a competitor (\\(i = 1,...,25\\)) and the first column (‚Äúobstacle_number‚Äù) is the last obstacle for which each was observed. The ‚Äúcensor‚Äù variable is the indicator of whether the obstacle was failed (1 = failed).\nWe will step through a few calculations to illustrate the Kaplan-Meier (KM) estimator. The calculation is easiest if the data is put in a format by the obstacle number (time) when things occurred. The code below produces this format:\n\nfake_data_summary &lt;- fake_data |&gt; \n  group_by(obstacle_number) |&gt; \n  summarize(fails = sum(censor == 1),\n            censored = sum(censor == 0)) |&gt;\n  ungroup() \n\nfake_data_summary\n\n# A tibble: 5 √ó 3\n  obstacle_number fails censored\n            &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;\n1               1     4        0\n2               2     3        4\n3               3     7        0\n4               4     2        0\n5               5     3        2\n\n\nWe see that each obstacle (time) had events. For the first obstacle, these events were all failures (4 fell). At obstacle 2, there were 3 failures but 4 censored. The censored competitors ran out of time before they could either complete or fail obstacle 3. We similarly see failures and censored observations at the last obstacle. Those censored at the last obstacle were those who completed the course.\nSuppose we wanted to calculate the Kaplan-Meier estimate of surviving past obstacle 2 we would need to find the following probabilities:\n\\(P(T &gt; 1 | T &gt; 0) = P(T &gt; 1) = 1 - \\frac{\\text{number of competitors that failed at obstacle 1}}{\\text{number of competitors that attempted obstacle 1}}\\)\n\\(P(T &gt; 2 | T &gt; 1) = 1 - \\frac{\\text{number of competitors that failed at obstacle 2}}{\\text{number of competitors that attempted obstacle 2}}\\)\nBelow we calculate the first probability:\n\nfake_data_summary &lt;- fake_data_summary |&gt; \n  mutate(at_risk = c(25,rep(NA,4)),\n         p_surv_cond = (at_risk - fails)/at_risk,\n         p_surv_km = p_surv_cond*1)\n\nfake_data_summary\n\n# A tibble: 5 √ó 6\n  obstacle_number fails censored at_risk p_surv_cond p_surv_km\n            &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;   &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1               1     4        0      25        0.84      0.84\n2               2     3        4      NA       NA        NA   \n3               3     7        0      NA       NA        NA   \n4               4     2        0      NA       NA        NA   \n5               5     3        2      NA       NA        NA   \n\n\nWe compute the probability using the formula earlier:\n\\(P(T &gt; t_i | T \\geq t_{i-1}) = \\frac{n_i - d_i}{n_i}\\)\nWe create a column for the number at risk (\\(n_1\\)) which is 25 for the first obstacle. \\(d_i\\) is found already in the ‚Äúfails‚Äù column.\nIn order to compute the second probability, we need to compute the next at risk value, \\(n_2\\). This value is 25 minus the 4 failures or 21. Since 0 were censored, we do not lose any other competitors. From there, we can again compute the conditional probability using the formula:\n\nfake_data_summary &lt;- fake_data_summary |&gt; \n  mutate(at_risk = c(25, 21, rep(NA,3)),\n         p_surv_cond = (at_risk - fails)/at_risk)\n\nfake_data_summary\n\n# A tibble: 5 √ó 6\n  obstacle_number fails censored at_risk p_surv_cond p_surv_km\n            &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;   &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1               1     4        0      25       0.84       0.84\n2               2     3        4      21       0.857     NA   \n3               3     7        0      NA      NA         NA   \n4               4     2        0      NA      NA         NA   \n5               5     3        2      NA      NA         NA   \n\n\nFinally, we then need to multiply these two probabilities together to get the Kaplan-Meier estimate of surviving past obstacle 2.\n\\(P(T &gt; 2) = P(T &gt; 1) * P(T &gt; 2 | T &gt; 1)\\)\nThe following code calculates the Kaplan-Meier estimate of surviving past obstacle 2:\n\nfake_data_summary$p_surv_km[2] &lt;- fake_data_summary$p_surv_cond[1]*\n  fake_data_summary$p_surv_cond[2]\nfake_data_summary\n\n# A tibble: 5 √ó 6\n  obstacle_number fails censored at_risk p_surv_cond p_surv_km\n            &lt;dbl&gt; &lt;int&gt;    &lt;int&gt;   &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1               1     4        0      25       0.84       0.84\n2               2     3        4      21       0.857      0.72\n3               3     7        0      NA      NA         NA   \n4               4     2        0      NA      NA         NA   \n5               5     3        2      NA      NA         NA   \n\n\nThe Kaplan-Meier estimate of surviving past obstacle 2 in this fake example is 0.72.\n\n\n\n\n\n\nExercise 2: Fake Data Kaplan-Meier Estimates\n\n\n\nIn this exercise you will calculate the remaining three Kaplan-Meier estimates of surviving past each obstacle for the fake data. Important note: how many are at risk for the next obstacle (3) in the table? 21 attempted obstacle 2 and three failed. However, 4 ran out of time and were censored! Thus, the correct number at risk, \\(n_3\\) is \\(21 - 7 = 14\\)! Use this value to help you complete the table."
  },
  {
    "objectID": "anw/kaplan_meier/index.html#kaplan-meier-estimator-manual-calculation",
    "href": "anw/kaplan_meier/index.html#kaplan-meier-estimator-manual-calculation",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analysis",
    "section": "Kaplan-Meier Estimator Manual Calculation",
    "text": "Kaplan-Meier Estimator Manual Calculation\nThe ninja data frame contains information about individual competitors in the ninja competition. We will need to summarize the data to calculate the Kaplan-Meier estimator manually.\n\n\n\n\n\n\nExercise 3: Manual Calculation of Kaplan-Meier Estimator\n\n\n\nIn this exercise you will calculate the Kaplan-Meier estimator of surviving past each obstacle in the ninja competition step-by-step.\n\n\n\n\n\n\nPart 1: Number of Events\n\n\n\nThe first step is to calculate the number of competitors that failed and the number of competitors that were censored at each point in time. These are the \\(d_i\\) and \\(c_i\\) values needed to calculate the conditional probability of surviving past each point in time.\nUse the following code to sum the number of competitors that failed and the number of competitors that were censored at each obstacle.\n\n\nninja_summary &lt;- ninja |&gt; \n  group_by(obstacle = obstacle_number) |&gt;\n  summarize(fails = sum(cause == \"Fall\"),\n            censored = sum(cause %in% c(\"Complete\", \"Time\")))\n\n\nAt which obstacle did the most competitors fail?\nAt which obstacle were the most competitors censored (not including obstacle 9 which is completion)?\n\n\n\n\n\n\n\n\n\nPart 2: At Risk Competitors\n\n\n\nThe second step is to calculate the number of competitors at risk at each point in time. This is the \\(n_i\\) value needed to calculate the conditional probability of surviving past each point in time.\nUse the following code to calculate the number of competitors at risk at each point in time from the ninja_summary data frame.\n\n\nninja_summary &lt;- ninja_summary |&gt; \n  mutate(attempts = 68 - lag(cumsum(fails), default = 0) - \n           lag(cumsum(censored), default = 0))\n\n\nWhich obstacle had the most competitors at risk?\nWhy don‚Äôt previously censored competitors contribute to the number of competitors at risk at the obstacle?\n\n\n\n\n\n\n\n\n\nPart 3: Conditional Survival Probability\n\n\n\nThe third step is to calculate the conditional probability of survival at each point in time. This is the \\(P(T \\geq t_i | T \\geq t_{i-1})\\) value needed to calculate the Kaplan-Meier estimator and is calculated as \\(1 - \\frac{d_i}{n_i}\\) or 1 minus the conditional ‚Äúfailure probability‚Äù.\n\nUse the ninja_summary data frame to calculate the probability that someone survives each obstacle. Do this using the mutate function to create a new column called surv_prob. Survival probability is 1 minus the number of competitors that failed divided by the number of competitors at risk. Save this data frame as ninja_summary.\n\n\n\nWhat percentage of at-risk competitors survived the first obstacle?\nWhat percentage of at-risk competitors failed the fifth obstacle?\nWhich obstacle had the highest conditional fail probability?\nDid obstacle 2 or obstacle 7 have a higher conditional survival rate?\n\n\n\n\n\n\n\n\n\nPart 4: Kaplan-Meier\n\n\n\nThe final step is to calculate the Kaplan-Meier estimator of surviving past each point in time (\\(\\hat{S}(t)\\)) This is calculated as the product of the conditional probabilities of surviving past each point in time up through the desired point in time (\\(\\prod_{i=1}^{t} P(T \\geq t_i | T \\geq t_{i-1})\\)).\nUse the following code to calculate the Kaplan-Meier estimator manually by multiplying the conditional probabilities of surviving past each point in time up through the desired point in time.\n\n\nninja_summary &lt;- ninja_summary |&gt; \n  mutate(km = cumprod(surv_prob))\n\n\nWhat is the Kaplan-Meier estimate for surviving past the first obstacle?\nWhat is the Kaplan-Meier estimate for surviving past first five obstacles?\nWhat is the farthest obstacle that for which the Kaplan-Meier estimator has more 50% of competitors surviving?\n\n\n\n\nPlotting the Kaplan-Meier Estimator\nWe will now use ggplot2 to plot the Kaplan-Meier estimator for the ninja competitors. The Kaplan-Meier estimator is a step function, so we will use geom_step to plot the estimator. We will also use geom_point to plot the points where the estimator changes.\n\n\n\n\n\n\n\nPart 5: Plotting the Kaplan-Meier Estimator\n\n\n\n\n\nUse the ninja_summary data frame in conjunction with ggplot2‚Äôs geom_step and geom_point to plot the Kaplan-Meier estimator for the ninja competitors.\nComment on the plot.\nWhat do you notice about where the lowest point on the plot is in regard to survival probability? Does survival probability reach zero? Why or why not?\n\n\n\n\n\n\n Note: The censored column is the number of competitors that were not tracked after that obstacle. For any obstacle that is not the last one, the number censored are the amount that ran out of time on the next obstacle. For the last obstacle, the number censored are the competitors that completed the course.\n\n\nNote: The lag function shifts the cumsum of the fails column down one row. The default = 0 argument fills in the first row with 0. This is necessary to help calculate the number of competitors at risk at each obstacle. Note that the lag function is not used in conjunction with the cumsumfunction for the censored column.\n\n TIP: You can pipe the data frame into the mutate function to create a new column.\nTIP: The mutate function works like so: data_frame |&gt; mutate(new_column = calculation)\n\n Note: The cumprod function calculates the cumulative product of the values given to it.\n\n Type ?geom_step, ?geom_point, or ?ggplot in the console to learn more about these functions.\n\nTIP: Remember that you can add the + operator to continue adding layers to the plot like seen below\n\nggplot(your_data, aes(x = time_var, y = kaplan_meier_var)) +\n  geom_step() +\n  geom_point()\n\nTIP: You can also add labels to the plot using the labs function like seen below\n\nggplot(your_data, aes(x = time_var, y = kaplan_meier_var)) +\n  geom_step() +\n  geom_point() +\n  labs(title = \"Your Title\",\n       x = \"X Axis Label\",\n       y = \"Y Axis Label\")"
  },
  {
    "objectID": "anw/kaplan_meier/index.html#using-r-packages-to-automatically-calculate-the-kaplan-meier-estimator",
    "href": "anw/kaplan_meier/index.html#using-r-packages-to-automatically-calculate-the-kaplan-meier-estimator",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analysis",
    "section": "Using R Packages to Automatically Calculate the Kaplan-Meier Estimator",
    "text": "Using R Packages to Automatically Calculate the Kaplan-Meier Estimator\nPhew! That was a lot of tedious work to calculate and plot the Kaplan-Meier estimator manually. Luckily, there is a much easier way to calculate the Kaplan-Meier estimator using R.\nThe survival package in R provides a function called survfit that can be used to calculate the Kaplan-Meier estimator. The survfit function requires a Surv object as input. The Surv object is created using the Surv function, which requires two arguments:\n\nThe time to event data. The time to event data is the time at which the event occurred or the time at which the individual was censored. In our case this is the obstacle_number in our ninja data.\nThe event status. The event status is a binary variable that indicates whether the event occurred or the individual was censored. The event status is coded as 1 if the event occurred and 0 if the individual was censored. This is contained in the censor column of the ninja data.\n\nBelow a survfit model is created for the ninja dataset and the results are stored in the ninja_km object.\n\nninja_km &lt;- survfit(Surv(obstacle_number, censor) ~ 1, data = ninja)\n\n\n\n\n\n\n\nExercise 4: Kaplan-Meier Estimates and Interpretation\n\n\n\nUse summary(ninja_km) to view a summary of the Kaplan-Meier estimator.\n\nDo the values in the survival column match the values you calculated manually?\n\n\nThe output also shows the 95% confidence intervals.\n\nWhich obstacle number is the first point in time where a survival rate of less than .5 falls within the 95% confidence interval?\nWhat do you notice about the standard error as the time increases and more ninjas have been eliminated?\n\n\n\n\nThe computations for calculating the Confidence Interval for the K-M Estimate are fairly complex. The method most commonly used is called the log-log survival function and was proposed by Kalbfleisch and Prentice (2002). This function is computed by \\(ln(-ln[\\hat{S}(t)])\\) with variance derived from the delta method and calculated by \\[\n\\frac{1}{[ln(\\hat{S}(t))]^2}\\sum_{t_i\\leq{t}}\\frac{d_i}{n_i(n_i - d_i)}\n\\].\nThe endpoints for the confidence interval for the log-log survival function are therefore found by \\(ln(-ln[\\hat{S}(t)]) \\pm Z_{1-\\alpha / 2} SE [ln(-ln[\\hat{S}(t)]) ]\\)\nAnd the endpoints expressed by the computer and seen in the summary are \\(exp[-exp(\\hat{c}_u)] \\text{ and } exp[-exp(\\hat{c}_l)]\\)\n\nQuartile Interpretation\nThe three quartiles are common statistics to look at when doing a survival analysis. The interpretations of these are as follows:\n\n\nNote: If the data is uncensored the estimate is just the median of the data. If the data is censored, the KM estimate is used to find these by finding the time at which it drops below the percentile\n\n25th Percentile- 75% of the people survive past this point in time\nMedian- 50% of the people will survive past this time\n75th Percentile- 25% survive past this time\n\n\n\n\n\n\n\nExercise 5: Interpreting Quartiles\n\n\n\nUse the results from quantile(ninja_km) to answer the following questions\n\nWhat is the earliest time that the confidence intervals imply that the true mean of surviving past that time could be 75%? What is the latest time?\nWhat is the interpretation of the NA values in the 75th percentile columns?\nWhat is the earliest time (within the 95% confidence interval) at which the true survival rate suggests 50% of the competitors would fail on or before?\n\n\n\n\n\nPlotting with R\nAfter fitting a Kaplan-Meier model, we can use the ggsurvplot function from the survminer package to plot the Kaplan-Meier estimator. The ggsurvplot function requires the Kaplan-Meier model as input.\nBelow is an example of how easy it is to plot the Kaplan-Meier estimator using R.\n\nggsurvplot(ninja_km,\n           conf.int = TRUE)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n‚Ñπ The deprecated feature was likely used in the ggpubr package.\n  Please report the issue at &lt;https://github.com/kassambara/ggpubr/issues&gt;.\n\n\nIgnoring unknown labels:\n‚Ä¢ fill : \"Strata\"\nIgnoring unknown labels:\n‚Ä¢ fill : \"Strata\""
  },
  {
    "objectID": "anw/kaplan_meier/index.html#the-log-rank-test-optionaladvanced",
    "href": "anw/kaplan_meier/index.html#the-log-rank-test-optionaladvanced",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analysis",
    "section": "The Log-Rank Test (optional/advanced)",
    "text": "The Log-Rank Test (optional/advanced)\nThe Log-Rank Test is a statistical test used to compare the survival probabilities of two or more groups. The test is used to determine if there is a statistically significant difference between the survival probabilities of the groups.\nThe hypotheses for our log-rank test are as follows:\n\n\\(H_0: S_M(t) = S_F(t)\\) for all \\(t\\)\n\\(H_a: S_M(t) \\neq S_F(t)\\) for at least one \\(t\\)\n\nwhere \\(S_M(t)\\) is the survival probability for males at time \\(t\\) and \\(S_F(t)\\) is the survival probability for females at time \\(t\\).\nWhen comparing two groups like this, we can calculate the expected number of deaths in each group. Below is the formula for calculating the number of expected deaths for group 0 at time \\(t_i\\):\n\\[\\hat{e}_{0i} = \\frac{n_{0i}d_i}{n_i}\\]\nwhere \\(n_{0i}\\) is the number of individuals at risk in group 0 at time \\(t_i\\), \\(d_i\\) is the total number of deaths at time \\(t_i\\), and \\(n_i\\) is the total number of individuals at risk at time \\(t_i\\).\nThe variance estimator is drawn from the hypergeometric distribution. The formula for the variance of the number of deaths in group 0 at time \\(t_i\\) is:\n\\[\\hat{v}_{0i} = \\frac{n_{0i}n_{1i}d_i(n_i - d_i)}{n_i^2(n_i - 1)}\\]\nwhere \\(n_{0i}\\) is the number of individuals at risk in group 0 at time \\(t_i\\), \\(n_{1i}\\) is the number of individuals at risk in group 1 at time \\(t_i\\), \\(d_i\\) is the total number of deaths at time \\(t_i\\), and \\(n_i\\) is the total number of individuals at risk at time \\(t_i\\).\nThe test statistic is calculated as the square of the sum of the differences between the observed and expected number of deaths for the group divided by the sum of the variance of the number of deaths for the group at each time point. The formula for the test statistic is as follows:\n\\[Q = \\frac{[\\sum_{i=1}^m (d_{0i} - \\hat{e}_{0i})]^2}{\\sum_{i=1}^m \\hat{v}_{0i}}\\]\nUsing the null hypothesis, the p-value can be calculated using the chi-squared distribution with 1 degree of freedom.\n\\[p = P(X^2(1) &gt; Q)\\]\n\n\nNOTE: This use of the chi-squared distribution assumes that the censoring is independent of the group.\n\nNOTE: The degrees of freedom for the chi-squared distribution is 1 because we are comparing two groups. If we were comparing more than two groups, the degrees of freedom would be the number of groups minus 1.\n\nThankfully R has a built-in function to perform the log-rank test. The survdiff function in the survival package can be used to perform the log-rank test. The survdiff function requires a Surv object as input. It will then perform the log-rank test and return the test statistic and p-value.\n\n\nNOTE: The log-rank test is a non-parametric test. This means that it does not assume that the data is normally distributed.\nThe code below runs the log-rank test on the ninja data set to compare the survival of male and female competitors.\n\nninja_km_diff &lt;- survdiff(Surv(obstacle_number,\n                               censor) ~ sex, data = ninja)\n\n\n\n\n\n\n\nExercise 7: Comparing Survival vs.¬†Expected\n\n\n\nUse the code below to see the results of the survdiff function\n\nninja_km_diff\n\nCall:\nsurvdiff(formula = Surv(obstacle_number, censor) ~ sex, data = ninja)\n\n       N Observed Expected (O-E)^2/E (O-E)^2/V\nsex=F 12       10     4.05      8.72      11.1\nsex=M 56       27    32.95      1.07      11.1\n\n Chisq= 11.1  on 1 degrees of freedom, p= 9e-04 \n\n\n\nHow many female competitors are in the data set? How many fell? How many were expected to fall (round to the nearest whole number)?\nDid more or less male competitors fall than expected?\nWhat is the p-value of the test? What does this mean?"
  },
  {
    "objectID": "anw/kaplan_meier/index.html#other-nonparametric-tests-optionaladvanced",
    "href": "anw/kaplan_meier/index.html#other-nonparametric-tests-optionaladvanced",
    "title": "American Ninja Warrior - Kaplan-Meier Survival Analysis",
    "section": "Other Nonparametric Tests (optional/advanced)",
    "text": "Other Nonparametric Tests (optional/advanced)\nAlthough the survdiff function uses the most common test for comparing Kaplan-Meier curves, there are a variety of other methods that can be used. These other methods developed because of the log rank test‚Äôs greatest weakness: It weights all time points equally even though there are fewer people at risk later than at the beginning. These methods are all similar to a standard log-rank test but attempt to weight time points in order to detect differences better throughout time as opposed to the end, which is where the log-rank test finds most of its differences. The ratio of the observed and expected number of deaths is calculated in a similar manner but with weights applied as seen below:\n\\[Q = \\frac{[\\sum_{i=1}^m w_i(d_0i - \\hat{e}_{0i})]^2}{\\sum_{i=1}^m w_i^2\\hat{v}_{0i}}\\]\nBelow some of the other methods that can be used are broken down, with their weighting and purpose explained:\n\nWilcoxon (Gehan-Breslow) Test: This test gives more weight to early time points based on the number of individuals at risk. Its weighting is: \\[w_i = n_i\\]\nTarone-Ware Test: This test gives more weight to time points with more individuals at risk, but less heavily than the Gehan-Breslow test. Its weighting is: \\[w_i = \\sqrt{n_i}\\]\nPeto-Prentice Test: This test also gives more weight to earlier time points, but not as much as the Gehan-Breslow test. Its weighting is:\n\n\\[w_i = \\tilde{S}(t_{(i)})\\] where \\[\\tilde{S}(t_{(i)}) = \\prod_{t_{(j)}&lt;t} \\left(1 - \\frac{d_j}{n_j}\\right)\\]\n\nFleming-Harrington Test: This test allows the user to chose \\(\\rho\\) and \\(q\\) values to weight the time points. If \\(\\rho\\) is larger it will weight the earlier time points more heavily, and if \\(q\\) is larger it will weight the later time points more heavily. Its weighting is:\n\n\\[w_i = [\\tilde{S}(t_{(i-1)})]^{\\rho}[1 - \\tilde{S}(t_{(i-1)})]^q\\] where \\[\\tilde{S}(t_{(i- 1)}) = \\text{Kaplan-Meier Estimate at time } t_{i-1}\\]\nThankfully the surv_pvalue function in the survminer package can be used to calculate the p-value for all of these tests by changing the method argument. See the table below for the different method arguments to use:\n\n\n\nTest\nMethod Argument\n\n\n\n\nLog Rank Test\nDefault- no argument needed\n\n\nWilcoxon/Gehan-Breslow\nmethod = ‚Äún‚Äù\n\n\nTarone-Ware\nmethod = ‚ÄúTW‚Äù\n\n\nPeto-Prentice\nmethod = ‚ÄúPP‚Äù\n\n\nFleming-Harrington\nmethod = ‚ÄúFH‚Äù\n\n\n\nThe surv_pvalue function does need a survfit object as input. We can use the ninja_km_gender object created earlier to check the p-values for the different methods.\n\n\n\n\n\n\nExercise 8: Log-Rank Tests\n\n\n\nRun the code below to see the p-values for the different methods.\n\nsurv_pvalue(ninja_km_gender) #log rank\nsurv_pvalue(ninja_km_gender, method = \"n\") #Gehan Breslow (generalized Wilcoxon)\nsurv_pvalue(ninja_km_gender, method = \"TW\") #tarone-ware\nsurv_pvalue(ninja_km_gender, method = \"PP\") #Peto-Prentice\nsurv_pvalue(ninja_km_gender, method = \"FH\") #Fleming-Harrington\n\n\nUsing \\(\\alpha = 0.05\\), do all of the tests lead to the same conclusion? If so what is the conclusion? If not which ones agree and which ones do not?\nWhich test had the smallest p-value?\nWhich test had the largest p-value?\nBased off of the p-values for the different tests, would you conclude that the difference between the genders is most likely more significant at the beginning or end of the course?"
  },
  {
    "objectID": "baseball/mlb_prime_age/index.html",
    "href": "baseball/mlb_prime_age/index.html",
    "title": "What‚Äôs the prime age of an MLB player?",
    "section": "",
    "text": "Methods/Facilitation notes\n\n\n\n\n\n\nThis module would be suitable for an in-class lab or take-home assignment in an introductory data science course that uses R.\nIt assumes a basic familiarity with the RStudio Environment and basic introduction to the tidyverse has already been covered, but tips on tidyverse code are provided throughout.\nStudents should be provided with the following data files (.csv) and Quarto document (.qmd) to produce visualizations and write up their answers to each exercise. Their final deliverable is to turn in an .html document produced by ‚ÄúRendering‚Äù the .qmd.\n\nbatter_stats.csv\npitcher_stats.csv\nStudent Quarto template\n\nPosit Cloud (via an Instructor account) or Github classroom are good options for disseminating files to students, but simply uploading files to your university‚Äôs course management system works, too."
  },
  {
    "objectID": "baseball/mlb_prime_age/index.html#batters",
    "href": "baseball/mlb_prime_age/index.html#batters",
    "title": "What‚Äôs the prime age of an MLB player?",
    "section": "Batters",
    "text": "Batters\n\n\n\n\n\n\nResearch question\n\n\n\nWhat is the average age a batter in the MLB reaches his prime?\n\n\nLet‚Äôs first note how our data is organized:\n\nhead(batter_stats)\n\n# A tibble: 6 √ó 18\n  x_mlbamid season team_name bats  player_name       age   war     g    ab    pa\n      &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    545361   2014 LAA       R     Mike Trout         22  8.29   157   602   705\n2    457763   2014 SFG       R     Buster Posey       27  7.52   147   547   605\n3    518960   2014 MIL       R     Jonathan Lucroy    28  7.44   153   585   655\n4    457705   2014 PIT       R     Andrew McCutch‚Ä¶    27  7.40   146   548   648\n5    519317   2014 MIA       R     Giancarlo Stan‚Ä¶    24  6.85   145   539   638\n6    488726   2014 CLE       L     Michael Brantl‚Ä¶    27  6.53   156   611   676\n# ‚Ñπ 8 more variables: h &lt;dbl&gt;, x1b &lt;dbl&gt;, x2b &lt;dbl&gt;, x3b &lt;dbl&gt;, hr &lt;dbl&gt;,\n#   r &lt;dbl&gt;, rbi &lt;dbl&gt;, best_war &lt;dbl&gt;\n\n\n\n\n\nCODE TIP: The function head() returns the first 6 rows of a dataset, and the function tail() returns the last 6. You can add the argument n = to display a different number of rows. Note these are base R functions and do not require the tidyverse to use.\nIf we arrange by x_mlbaid we can see that there can be multiple observations per player, where each row represents a different season.\n\nbatter_stats |&gt; \n  arrange(x_mlbamid) |&gt; \n  slice(1:10)\n\n# A tibble: 10 √ó 18\n   x_mlbamid season team_name bats  player_name    age     war     g    ab    pa\n       &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1    110029   2014 NYM       L     Bobby Abreu     40 -0.213     78   133   155\n 2    112526   2014 NYM       R     Bartolo Col‚Ä¶    41 -0.588     31    62    69\n 3    112526   2015 NYM       R     Bartolo Col‚Ä¶    42 -0.0408    33    58    64\n 4    112526   2016 NYM       R     Bartolo Col‚Ä¶    43 -0.243     34    60    65\n 5    112526   2017 - - -     R     Bartolo Col‚Ä¶    44 -0.266     28    19    20\n 6    112526   2018 TEX       R     Bartolo Col‚Ä¶    45 -0.0475    28     4     4\n 7    114739   2014 CLE       L     Jason Giambi    43 -0.496     26    60    70\n 8    115629   2014 COL       R     LaTroy Hawk‚Ä¶    41 -0.0141    57     1     1\n 9    115629   2015 - - -     R     LaTroy Hawk‚Ä¶    42 -0.0146    42     1     1\n10    116338   2014 DET       R     Torii Hunter    38  1.11     142   549   586\n# ‚Ñπ 8 more variables: h &lt;dbl&gt;, x1b &lt;dbl&gt;, x2b &lt;dbl&gt;, x3b &lt;dbl&gt;, hr &lt;dbl&gt;,\n#   r &lt;dbl&gt;, rbi &lt;dbl&gt;, best_war &lt;dbl&gt;\n\n\n\n\n¬†The pipe: Recall that |&gt; is called the ‚Äúpipe‚Äù function and can be read as ‚Äúand then.‚Äù In English, the code on the left can be read as ‚Äútake the batter_stats data and then arrange it by x_mlbamid and then slice the first 10 rows.‚Äù Mathematically, the pipe accomplishes f(g(x)) with the (psudeo-)code x |&gt; g() |&gt; f(). Read more about the pipe here.\n\ndplyr: arrange() and slice() are examples of dplyr verbs: tidyverse functions that do something to / act on the data. Other examples include filter(), select(), mutate(), group_by(), summarize(), relocate(), and many more. These verbs are often chained together with the pipe to accomplish multiple data wrangling tasks. Read more about data wrangling with dplyr here.\n\n\n\n\n\n\n\nExercise 1:\n\n\n\nWhich seasons are included in this data?\n\n\n\n\nTIP: Try writing your answer as a full sentence in the .qmd using inline code. For example, if you have the first season saved in an object first_season, then including `r first_season` outside a code chunk will allow you to auto-populate this value in a sentence.\n\n\n\n\n\n\nImportant\n\n\n\nIn order to determine the prime age of each player, we need to look for the year in which his war reached its player-specific maximum. We can utilize the group_by() function to do this.\n\n\n\n\n\n\n\n\nExercise 2:\n\n\n\nCopy, paste the following code and fill in the blanks to create a new variable best_war that contains a player‚Äôs maximum war.\n\nbatter_stats &lt;- batter_stats |&gt; \n  group_by(________) |&gt; \n  mutate(_______ = _______(_______)) |&gt; \n  ungroup()\n\n\n\n\n\nCODE TIP: group_by() allows all subsequent actions to be done for each group of the grouping variable. Therefore, if we group by player id, we‚Äôre able to determine the maximum war for each player, not simply the maximum war for the whole dataset. It‚Äôs often a good idea to ungroup() at the end of a chain of code, otherwise the next time you try to use your data, it will still perform every operation by group.\nTake a quick glimpse() of your data to confirm the first few values of best_war match those below before proceeding.\n\n\nRows: 13,917\nColumns: 18\n$ x_mlbamid   &lt;dbl&gt; 545361, 457763, 518960, 457705, 519317, 488726, 543685, 43‚Ä¶\n$ season      &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014‚Ä¶\n$ team_name   &lt;chr&gt; \"LAA\", \"SFG\", \"MIL\", \"PIT\", \"MIA\", \"CLE\", \"WSN\", \"TOR\", \"P‚Ä¶\n$ bats        &lt;chr&gt; \"R\", \"R\", \"R\", \"R\", \"R\", \"L\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\"‚Ä¶\n$ player_name &lt;chr&gt; \"Mike Trout\", \"Buster Posey\", \"Jonathan Lucroy\", \"Andrew M‚Ä¶\n$ age         &lt;dbl&gt; 22, 27, 28, 27, 24, 27, 24, 33, 31, 35, 28, 28, 31, 23, 30‚Ä¶\n$ war         &lt;dbl&gt; 8.2866, 7.5222, 7.4368, 7.4014, 6.8473, 6.5310, 6.4054, 6.‚Ä¶\n$ g           &lt;dbl&gt; 157, 147, 153, 146, 145, 156, 153, 155, 111, 148, 148, 158‚Ä¶\n$ ab          &lt;dbl&gt; 602, 547, 585, 548, 539, 611, 613, 553, 379, 549, 574, 608‚Ä¶\n$ pa          &lt;dbl&gt; 705, 605, 655, 648, 638, 676, 683, 673, 460, 614, 644, 695‚Ä¶\n$ h           &lt;dbl&gt; 173, 170, 176, 172, 155, 200, 176, 158, 110, 178, 163, 155‚Ä¶\n$ x1b         &lt;dbl&gt; 89, 118, 108, 103, 86, 133, 110, 96, 79, 125, 102, 93, 134‚Ä¶\n$ x2b         &lt;dbl&gt; 39, 28, 53, 38, 31, 45, 39, 27, 20, 33, 34, 31, 37, 37, 34‚Ä¶\n$ x3b         &lt;dbl&gt; 9, 2, 2, 6, 1, 2, 6, 0, 0, 1, 4, 2, 2, 9, 1, 1, 2, 1, 3, 1‚Ä¶\n$ hr          &lt;dbl&gt; 36, 22, 13, 25, 37, 20, 21, 35, 11, 19, 23, 29, 14, 16, 19‚Ä¶\n$ r           &lt;dbl&gt; 115, 72, 73, 89, 89, 94, 111, 101, 45, 79, 95, 93, 77, 92,‚Ä¶\n$ rbi         &lt;dbl&gt; 111, 89, 69, 83, 105, 97, 83, 103, 67, 77, 73, 98, 82, 69,‚Ä¶\n$ best_war    &lt;dbl&gt; 9.4559, 7.5222, 7.4368, 7.4014, 6.8473, 6.5310, 6.7801, 6.‚Ä¶\n\n\n\n\nCODE TIP: In real life data science work, you won‚Äôt usually be provided with the ‚Äúcorect‚Äù answer to compare to, so it‚Äôs often a good idea to do a quick check after any data transformation to make sure your code did what you expected. In this case, you might choose one player to verify that their best_war value is in fact equal to their maximum war value. You can do a quick filter for that player in your console, or use the search feature when Viewing the full data in spreadsheet view.\n\n\n\n\n\n\nExercise 3:\n\n\n\nCreate a new dataset called prime_age that keeps only the rows where a player‚Äôs war is equal to his best_war.\nWhat are the dimensions of this new dataset?\n\n\n\n\n\nHint: what dyplr verb do you need to keep rows that meet a criteria?\nIdeally, we want there to be one row per player in our new dataset. However, if we check the number of unique players we have in our original data, we find this does not match the number of rows in prime_age.\n\n\nCODE TIP: Two options for checking the number of unique levels of a variable are length(unique(data$variable)) or data |&gt; distinct(variable) |&gt; nrow()\n\n\n\n\n\n\nExercise 4:\n\n\n\nReport the number of unique players in the dataset batter_stats.\nInspect the prime_age data more closely. What is the maximum number of rows that appear for a player in this dataset? Comment on why this is happening. Hint: creating a new variable that counts the number of rows per id can help you investigate this.\n\n\n\n\nCODE TIP: group_by(grouping_variable) followed by mutate(n = n()) will count the number of rows per level of the grouping variable.\n\n\n\n\n\n\nExercise 5:\n\n\n\nDetermine a strategy for reducing prime_age down to one row per person (still maintaining all relevant columns). Describe your strategy in words and then write code to accomplish it. Careful - don‚Äôt just arbitrarily throw away rows! There are multiple ways you might approach this, but you should justify your decision(s) and think through implications for your ultimate analysis goal: estimating prime age.\n\n\nYour reduced prime_age should look something like this:\n\n\nRows: 3,752\nColumns: 18\n$ x_mlbamid   &lt;dbl&gt; 457763, 518960, 457705, 519317, 488726, 430832, 431145, 13‚Ä¶\n$ season      &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014‚Ä¶\n$ team_name   &lt;chr&gt; \"SFG\", \"MIL\", \"PIT\", \"MIA\", \"CLE\", \"TOR\", \"PIT\", \"TEX\", \"M‚Ä¶\n$ bats        &lt;chr&gt; \"R\", \"R\", \"R\", \"R\", \"L\", \"R\", \"R\", \"R\", \"R\", \"R\", \"L\", \"L\"‚Ä¶\n$ player_name &lt;chr&gt; \"Buster Posey\", \"Jonathan Lucroy\", \"Andrew McCutchen\", \"Gi‚Ä¶\n$ age         &lt;dbl&gt; 27, 28, 27, 24, 27, 33, 31, 35, 28, 23, 30, 24, 27, 35, 29‚Ä¶\n$ war         &lt;dbl&gt; 7.5222, 7.4368, 7.4014, 6.8473, 6.5310, 6.1703, 6.1427, 5.‚Ä¶\n$ g           &lt;dbl&gt; 147, 153, 146, 145, 156, 155, 111, 148, 148, 148, 156, 140‚Ä¶\n$ ab          &lt;dbl&gt; 547, 585, 548, 539, 611, 553, 379, 549, 574, 558, 563, 524‚Ä¶\n$ pa          &lt;dbl&gt; 605, 655, 648, 638, 676, 673, 460, 614, 644, 640, 643, 616‚Ä¶\n$ h           &lt;dbl&gt; 170, 176, 172, 155, 200, 158, 110, 178, 163, 165, 150, 150‚Ä¶\n$ x1b         &lt;dbl&gt; 118, 108, 103, 86, 133, 96, 79, 125, 102, 103, 96, 89, 103‚Ä¶\n$ x2b         &lt;dbl&gt; 28, 53, 38, 31, 45, 27, 20, 33, 34, 37, 34, 28, 35, 37, 18‚Ä¶\n$ x3b         &lt;dbl&gt; 2, 2, 6, 1, 2, 0, 0, 1, 4, 9, 1, 1, 2, 1, 1, 3, 1, 7, 3, 2‚Ä¶\n$ hr          &lt;dbl&gt; 22, 13, 25, 37, 20, 35, 11, 19, 23, 16, 19, 32, 36, 16, 21‚Ä¶\n$ r           &lt;dbl&gt; 72, 73, 89, 89, 94, 101, 45, 79, 95, 92, 87, 89, 80, 85, 7‚Ä¶\n$ rbi         &lt;dbl&gt; 89, 69, 83, 105, 97, 103, 67, 77, 73, 69, 74, 78, 107, 82,‚Ä¶\n$ best_war    &lt;dbl&gt; 7.5222, 7.4368, 7.4014, 6.8473, 6.5310, 6.1703, 6.1427, 5.‚Ä¶\n\n\n\n\n\n\n\n\nExercise 6:\n\n\n\nProduce a visualization that explores the distribution of prime ages, for all players in this data.\n\n\n\n\n\n\n\n\nExercise 7:\n\n\n\nBased on the graph, ‚Äúeyeball‚Äù an initial answer to the research question: at what age do professional batters tend to be at their ‚Äúprime‚Äù?\n\n\n\n\n\n\n\n\nExercise 8:\n\n\n\nCalculate the mean and the median prime age for batters in this data.\n\n\n\n\n\n\n\n\nExercise 9:\n\n\n\nReproduce your graph from above but add 2 lines to the graph representing the mean and median of the distribution.\n\n\n\n\n\nTip: Add a layer called geom_vline to your ggplot code. Make sure the colors of the lines are different."
  },
  {
    "objectID": "baseball/mlb_prime_age/index.html#pitchers",
    "href": "baseball/mlb_prime_age/index.html#pitchers",
    "title": "What‚Äôs the prime age of an MLB player?",
    "section": "Pitchers",
    "text": "Pitchers\n\n\n\n\n\n\nResearch question\n\n\n\nWhat is the average age an MLB pitcher reaches his prime?\n\n\n\n\n\n\n\n\nExercise 10\n\n\n\nCopy, paste, tweak appropriate code from previous exercises to determine the prime age of pitchers, using the pitcher_stats data.\n\n\n\n\n\nCheck: there are 2382 unique pitchers in the pitcher_stats data, so your final dataset for analysis should have that many rows."
  }
]